{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=set(w.rstrip() for w in open('D:\\\\Tutorials\\\\Data Science Natural Language Processing (NLP) in Python\\\\NLP Data\\\\Sentiment Analyzer\\\\stopwords.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rev=BeautifulSoup(open('D:\\\\Tutorials\\\\Data Science Natural Language Processing (NLP) in Python\\\\NLP Data\\\\Sentiment Analyzer\\\\positive.reviews').read())\n",
    "pos_rev=pos_rev.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rev=BeautifulSoup(open('D:\\\\Tutorials\\\\Data Science Natural Language Processing (NLP) in Python\\\\NLP Data\\\\Sentiment Analyzer\\\\negative.reviews').read())\n",
    "neg_rev=neg_rev.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(pos_rev)\n",
    "pos_rev=pos_rev[:len(neg_rev)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    s=s.lower()\n",
    "    tokens=nltk.tokenize.word_tokenize(s)\n",
    "    tokens=[t for t in tokens if len(t)>2]\n",
    "    tokens=[wnl.lemmatize(t) for t in tokens]\n",
    "    tokens=[t for t in tokens if t not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenized=[]\n",
    "neg_tokenized=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_map={}\n",
    "current_index=0\n",
    "for review in pos_rev:\n",
    "    tokens=my_tokenizer(review.text)\n",
    "    pos_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token]=current_index\n",
    "            current_index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in neg_rev:\n",
    "    tokens=my_tokenizer(review.text)\n",
    "    neg_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token]=current_index\n",
    "            current_index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_vector(tokens,label):\n",
    "    x=np.zeros(len(word_index_map)+1)\n",
    "    for t in tokens:\n",
    "        i=word_index_map[t]\n",
    "        x[i]+=1\n",
    "    x=x/x.sum()\n",
    "    x[-1]=label\n",
    "    return x\n",
    "N=len(pos_tokenized)+len(neg_tokenized)\n",
    "data=np.zeros((N,len(word_index_map)+1))\n",
    "i=0\n",
    "for tokens in pos_tokenized:\n",
    "    xy=tokens_to_vector(tokens,1)\n",
    "    data[i,:]=xy\n",
    "    i+=1\n",
    "for tokens in neg_tokenized:\n",
    "    xy=tokens_to_vector(tokens,0)\n",
    "    data[i,:]=xy\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "X=data[:,:-1]\n",
    "y=data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set acc: 0.7079843662758235\n",
      "Test set acc: 0.68\n"
     ]
    }
   ],
   "source": [
    "X_train=X[:-100,]\n",
    "y_train=y[:-100,]\n",
    "X_test=X[-100:,]\n",
    "y_test=y[-100:,]\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "print('Train set acc:',model.score(X_train,y_train))\n",
    "print('Test set acc:',model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card -0.6059247616840455\n",
      "ha 0.6201122831377617\n",
      "month -0.7066564775485176\n",
      "space 0.5738421442401013\n",
      "you 0.9448811249848311\n",
      "time -0.5528530123246125\n",
      "using 0.687270086066657\n",
      "'ve 0.5161556416217755\n",
      "love 1.1345288723195721\n",
      "lot 0.659021868238785\n",
      "wa -1.664913422808285\n",
      "sound 1.2115581865877574\n",
      "little 0.9246519974592425\n",
      "n't -1.917322707035652\n",
      "buy -0.9134376950215942\n",
      "price 2.6489657886751767\n",
      "customer -0.5952165712274425\n",
      "look 0.5616482822422956\n",
      "money -1.0198165920275395\n",
      "size 0.546193499507105\n",
      "video 0.5620662374853169\n",
      "excellent 1.2873163121953284\n",
      "happy 0.6263504225271604\n",
      "home 0.5621951373697451\n",
      "unit -0.6732298080951128\n",
      "recommend 0.6201207763720699\n",
      "speaker 1.1095124743431277\n",
      "perfect 0.7746946324235422\n",
      "cable 0.6135062217789219\n",
      "picture 0.5689809923212316\n",
      "fast 1.0334637650158303\n",
      "doe -1.1489795460869987\n",
      "quality 1.4736773634893392\n",
      "easy 1.5555742534347712\n",
      "tried -0.7106899323146009\n",
      "paper 0.522033537197292\n",
      "highly 1.0009492961834245\n",
      "value 0.5266516707267199\n",
      "memory 0.81148223317687\n",
      "company -0.5068076671070832\n",
      "usb 0.5088900029584834\n",
      "bit 0.5841957039279119\n",
      "pretty 0.7127058656711913\n",
      "then -1.0062149357011723\n",
      "week -0.6937957208212567\n",
      "comfortable 0.6846134257493137\n",
      "support -0.8041201141367246\n",
      "bad -0.7492935817071203\n",
      "try -0.6348561515982616\n",
      "returned -0.740781787708055\n",
      "warranty -0.5621481561393782\n",
      "poor -0.7769862622925182\n",
      "stopped -0.516153566206441\n",
      "returning -0.5088653793779154\n",
      "waste -0.9356588521074966\n",
      "item -0.8737456584022671\n",
      "expected 0.5401924682125037\n",
      "return -1.1661215760914216\n",
      "refund -0.545187385099409\n",
      "junk -0.5032527696593005\n"
     ]
    }
   ],
   "source": [
    "threshold=0.5\n",
    "for word,index in word_index_map.items():\n",
    "    weight=model.coef_[0][index]\n",
    "    if weight>threshold or weight<-threshold:\n",
    "        print(word,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
